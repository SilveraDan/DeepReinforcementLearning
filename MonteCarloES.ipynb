{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo ES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Line World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: defaultdict(<function mc_es.<locals>.<lambda> at 0x110349790>, {3: array([0.99697885, 1.        ]), 2: array([0.99340102, 0.99849398]), 1: array([0.99457259, 0.99562142]), 0: array([0.99100899, 0.99569402])})\n",
      "Policy: {3: np.int64(1), 2: np.int64(1), 1: np.int64(1), 0: np.int64(1)}\n",
      "State: 1, World: - O - - -\n",
      "State: 0, Action: 1, Reward: 0, Next State: 1\n",
      "State: 2, World: - - O - -\n",
      "State: 1, Action: 1, Reward: 0, Next State: 2\n",
      "State: 3, World: - - - O -\n",
      "State: 2, Action: 1, Reward: 0, Next State: 3\n",
      "State: 4, World: - - - - O\n",
      "State: 3, Action: 1, Reward: 1, Next State: 4\n",
      "Episode finished in 4 steps.\n"
     ]
    }
   ],
   "source": [
    "# line_world.py\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class LineWorld:\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.start_state = 0\n",
    "        self.end_state = size - 1\n",
    "        self.reset()\n",
    "        self.action_space = namedtuple('ActionSpace', ['n'])\n",
    "        self.action_space.n = 2  # Two actions: 0 (left) and 1 (right)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 0:  # Move left\n",
    "            self.state = max(0, self.state - 1)\n",
    "        elif action == 1:  # Move right\n",
    "            self.state = min(self.size - 1, self.state + 1)\n",
    "        reward = 1 if self.state == self.end_state else 0\n",
    "        done = self.state == self.end_state\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"State: {self.state}, World: {' '.join(['O' if i == self.state else '-' for i in range(self.size)])}\")\n",
    "\n",
    "def mc_es(env, num_episodes, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done or len(episode) > 100:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy\n",
    "\n",
    "def visualize_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done or steps > 100:\n",
    "            print(f\"Episode finished in {steps} steps.\")\n",
    "            break\n",
    "\n",
    "# Test and visualize LineWorld\n",
    "if __name__ == \"__main__\":\n",
    "    env = LineWorld()\n",
    "    Q, policy = mc_es(env, num_episodes=1000)\n",
    "    print(\"Q-values:\", Q)\n",
    "    print(\"Policy:\", policy)\n",
    "    visualize_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Q-values montrent que l'action 1 (se déplacer à droite) a une valeur plus élevée dans chaque état, ce qui est cohérent car le but est d'atteindre l'état final le plus rapidement possible en se déplaçant à droite.\n",
    "L'agent atteint l'état final en 4 étapes, ce qui montre que la politique apprise est efficace pour ce problème simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: defaultdict(<function mc_es.<locals>.<lambda> at 0x11027f670>, {(4, 3): array([0.47770701, 1.        , 0.55238095, 0.64307692]), (4, 2): array([0.384     , 0.66464646, 0.44043321, 0.44277674]), (4, 1): array([0.33860759, 0.45954693, 0.35538462, 0.37070254]), (4, 0): array([0.34195402, 0.36927224, 0.30699088, 0.31545741]), (3, 0): array([0.34755332, 0.35164835, 0.29006623, 0.33461047]), (2, 0): array([0.3453159 , 0.36231884, 0.34256055, 0.33549784]), (2, 1): array([0.31806616, 0.40767386, 0.32723112, 0.42205323]), (1, 0): array([0.3830156 , 0.40778342, 0.3860262 , 0.39640411]), (0, 0): array([0.44234405, 0.46711328, 0.45626911, 0.44819129]), (1, 1): array([0.35299902, 0.4       , 0.367     , 0.39548577]), (0, 1): array([0.40996503, 0.44298605, 0.4023622 , 0.40448505]), (3, 2): array([0.34751773, 0.5641953 , 0.3777403 , 0.51880878]), (3, 1): array([0.3200569 , 0.42986425, 0.36      , 0.38905325]), (3, 3): array([0.43249428, 0.69347826, 0.4048913 , 0.64233577]), (2, 3): array([0.38571429, 0.52256944, 0.39779006, 0.51149425]), (1, 3): array([0.37825421, 0.44992948, 0.37538462, 0.47862069]), (1, 4): array([0.40557276, 0.4145937 , 0.41233766, 0.50425894]), (1, 2): array([0.34549878, 0.4217016 , 0.39427861, 0.42041312]), (0, 2): array([0.38469713, 0.42990654, 0.37513751, 0.42791762]), (0, 3): array([0.37612323, 0.38031915, 0.36294416, 0.44635762]), (2, 2): array([0.33286713, 0.44117647, 0.3543956 , 0.48866856]), (3, 4): array([0.55813953, 0.63571429, 0.49847095, 1.        ]), (2, 4): array([0.45530146, 0.45773196, 0.4194831 , 0.67073171]), (0, 4): array([0.36804565, 0.3847352 , 0.41405082, 0.42987805])})\n",
      "Policy: {(4, 3): np.int64(1), (4, 2): np.int64(1), (4, 1): np.int64(1), (4, 0): np.int64(1), (3, 0): np.int64(1), (2, 0): np.int64(1), (2, 1): np.int64(3), (1, 0): np.int64(1), (0, 0): np.int64(1), (1, 1): np.int64(1), (0, 1): np.int64(1), (3, 2): np.int64(1), (3, 1): np.int64(1), (3, 3): np.int64(1), (2, 3): np.int64(1), (1, 3): np.int64(3), (1, 4): np.int64(3), (1, 2): np.int64(1), (0, 2): np.int64(1), (0, 3): np.int64(3), (2, 2): np.int64(3), (3, 4): np.int64(3), (2, 4): np.int64(3), (0, 4): np.int64(3)}\n",
      "- - - - -\n",
      "O - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "Current State: (0, 1)\n",
      "State: (0, 0), Action: 1, Reward: 0, Next State: (0, 1)\n",
      "- - - - -\n",
      "- - - - -\n",
      "O - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "Current State: (0, 2)\n",
      "State: (0, 1), Action: 1, Reward: 0, Next State: (0, 2)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "O - - - -\n",
      "- - - - -\n",
      "Current State: (0, 3)\n",
      "State: (0, 2), Action: 1, Reward: 0, Next State: (0, 3)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- O - - -\n",
      "- - - - -\n",
      "Current State: (1, 3)\n",
      "State: (0, 3), Action: 3, Reward: 0, Next State: (1, 3)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - O - -\n",
      "- - - - -\n",
      "Current State: (2, 3)\n",
      "State: (1, 3), Action: 3, Reward: 0, Next State: (2, 3)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - O - -\n",
      "Current State: (2, 4)\n",
      "State: (2, 3), Action: 1, Reward: 0, Next State: (2, 4)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - O -\n",
      "Current State: (3, 4)\n",
      "State: (2, 4), Action: 3, Reward: 0, Next State: (3, 4)\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - -\n",
      "- - - - O\n",
      "Current State: (4, 4)\n",
      "State: (3, 4), Action: 3, Reward: 1, Next State: (4, 4)\n",
      "Episode finished in 8 steps.\n"
     ]
    }
   ],
   "source": [
    "# grid_world.py\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class GridWorld:\n",
    "    def __init__(self, width=5, height=5):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.start_state = (0, 0)\n",
    "        self.end_state = (width - 1, height - 1)\n",
    "        self.reset()\n",
    "        self.action_space = namedtuple('ActionSpace', ['n'])\n",
    "        self.action_space.n = 4  # Four actions: 0 (up), 1 (down), 2 (left), 3 (right)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0:  # Move up\n",
    "            y = max(0, y - 1)\n",
    "        elif action == 1:  # Move down\n",
    "            y = min(self.height - 1, y + 1)\n",
    "        elif action == 2:  # Move left\n",
    "            x = max(0, x - 1)\n",
    "        elif action == 3:  # Move right\n",
    "            x = min(self.width - 1, x + 1)\n",
    "        self.state = (x, y)\n",
    "        reward = 1 if self.state == self.end_state else 0\n",
    "        done = self.state == self.end_state\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        grid = [['-' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        x, y = self.state\n",
    "        grid[y][x] = 'O'\n",
    "        for row in grid:\n",
    "            print(' '.join(row))\n",
    "        print(f\"Current State: {self.state}\")\n",
    "\n",
    "def mc_es(env, num_episodes, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done or len(episode) > 100:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy\n",
    "\n",
    "def visualize_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done or steps > 100:\n",
    "            print(f\"Episode finished in {steps} steps.\")\n",
    "            break\n",
    "\n",
    "# Test and visualize GridWorld\n",
    "if __name__ == \"__main__\":\n",
    "    env = GridWorld()\n",
    "    Q, policy = mc_es(env, num_episodes=1000)\n",
    "    print(\"Q-values:\", Q)\n",
    "    print(\"Policy:\", policy)\n",
    "    visualize_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'agent atteint l'état final (4, 4) en 8 étapes. Cela montre que la politique apprise permet à l'agent de naviguer efficacement à travers la grille pour atteindre l'état final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: defaultdict(<function mc_es.<locals>.<lambda> at 0x11054a8b0>, {('rock',): array([ 0.,  1., -1.]), (): array([-0.03625378, -0.04166667,  0.        ]), ('paper',): array([-1.,  0.,  1.]), ('scissors',): array([ 1., -1.,  0.])})\n",
      "Policy: {('rock',): np.int64(1), (): np.int64(2), ('paper',): np.int64(2), ('scissors',): np.int64(0)}\n",
      "State: ['scissors'], Opponent Moves: ['scissors', None]\n",
      "State: (), Action: 2, Reward: 0, Next State: ('scissors',)\n",
      "State: ['scissors', 'rock'], Opponent Moves: ['scissors', 'scissors']\n",
      "State: ('scissors',), Action: 0, Reward: 1, Next State: ('scissors', 'rock')\n",
      "Episode finished in 2 steps.\n"
     ]
    }
   ],
   "source": [
    "# two_round_rps.py\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class TwoRoundRPS:\n",
    "    def __init__(self):\n",
    "        self.actions = ['rock', 'paper', 'scissors']\n",
    "        self.reset()\n",
    "        self.action_space = namedtuple('ActionSpace', ['n'])\n",
    "        self.action_space.n = 3  # Three actions: 0 (rock), 1 (paper), 2 (scissors)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = []\n",
    "        self.opponent_moves = [random.choice(self.actions), None]\n",
    "        return tuple(self.state)\n",
    "    \n",
    "    def step(self, action):\n",
    "        move = self.actions[action]\n",
    "        self.state.append(move)\n",
    "        if len(self.state) == 2:\n",
    "            self.opponent_moves[1] = self.state[0]\n",
    "            reward = self.get_reward(self.state[1], self.opponent_moves[1])\n",
    "            done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = False\n",
    "        return tuple(self.state), reward, done\n",
    "    \n",
    "    def get_reward(self, move, opponent_move):\n",
    "        if move == opponent_move:\n",
    "            return 0\n",
    "        elif (move == 'rock' and opponent_move == 'scissors') or \\\n",
    "             (move == 'paper' and opponent_move == 'rock') or \\\n",
    "             (move == 'scissors' and opponent_move == 'paper'):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"State: {self.state}, Opponent Moves: {self.opponent_moves}\")\n",
    "\n",
    "def mc_es(env, num_episodes, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done or len(episode) > 100:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy\n",
    "\n",
    "def visualize_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if len(state) == 2:\n",
    "            break\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done or steps > 100:\n",
    "            print(f\"Episode finished in {steps} steps.\")\n",
    "            break\n",
    "\n",
    "# Test and visualize Two Round Rock Paper Scissors\n",
    "if __name__ == \"__main__\":\n",
    "    env = TwoRoundRPS()\n",
    "    Q, policy = mc_es(env, num_episodes=1000)\n",
    "    print(\"Q-values:\", Q)\n",
    "    print(\"Policy:\", policy)\n",
    "    visualize_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Q-values montrent les récompenses attendues pour chaque action dans différents états. Par exemple, pour l'état ('rock',), l'action 1 (papier) a la récompense la plus élevée. Ici l'agent termine le jeu en 2 étapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monty Hall 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: defaultdict(<function mc_es.<locals>.<lambda> at 0x11054af70>, {(0, 1): array([0.46835443, 0.625     ]), (2, 0): array([0.30985915, 0.66666667]), (1, 0): array([0.30337079, 0.58510638]), (2, 1): array([0.4025974 , 0.73684211]), (1, 2): array([0.38636364, 0.7       ]), (0, 2): array([0.37647059, 0.63513514])})\n",
      "Policy: {(0, 1): np.int64(1), (2, 0): np.int64(1), (1, 0): np.int64(1), (2, 1): np.int64(1), (1, 2): np.int64(1), (0, 2): np.int64(1)}\n",
      "Doors: ['goat', 'car', 'goat'], Selected Door: 1, Revealed Door: 2\n",
      "State: (0, 2), Action: 1, Reward: 1, Next State: (0, 2)\n",
      "Episode finished in 1 steps.\n"
     ]
    }
   ],
   "source": [
    "# monty_hall_1.py\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class MontyHall1:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.action_space = namedtuple('ActionSpace', ['n'])\n",
    "        self.action_space.n = 2  # Two actions: 0 (stay), 1 (switch)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.doors = ['goat', 'goat', 'car']\n",
    "        random.shuffle(self.doors)\n",
    "        self.selected_door = random.randint(0, 2)\n",
    "        self.revealed_door = self.reveal_door()\n",
    "        self.state = (self.selected_door, self.revealed_door)\n",
    "        return self.state\n",
    "    \n",
    "    def reveal_door(self):\n",
    "        available_doors = [i for i in range(3) if i != self.selected_door and self.doors[i] == 'goat']\n",
    "        return random.choice(available_doors)\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 1:  # Switch\n",
    "            self.selected_door = [i for i in range(3) if i != self.selected_door and i != self.revealed_door][0]\n",
    "        reward = 1 if self.doors[self.selected_door] == 'car' else 0\n",
    "        done = True\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Doors: {self.doors}, Selected Door: {self.selected_door}, Revealed Door: {self.revealed_door}\")\n",
    "\n",
    "def mc_es(env, num_episodes, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done or len(episode) > 100:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy\n",
    "\n",
    "def visualize_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done or steps > 100:\n",
    "            print(f\"Episode finished in {steps} steps.\")\n",
    "            break\n",
    "\n",
    "# Test and visualize Monty Hall Level 1\n",
    "if __name__ == \"__main__\":\n",
    "    env = MontyHall1()\n",
    "    Q, policy = mc_es(env, num_episodes=1000)\n",
    "    print(\"Q-values:\", Q)\n",
    "    print(\"Policy:\", policy)\n",
    "    visualize_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Q-values montrent que l'action 1 (changer de porte) a une récompense plus élevée, ce qui est attendu dans l'environnement'.\n",
    "Pour ce qui est de la politique, elle suggère de toujours changer de porte (action 1), ce qui est optimal dans le problème de Monty Hall'. L'agent du coup suit la politique optimale et change de porte, gagnant la voiture en une seule étape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monty Hall 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: defaultdict(<function mc_es.<locals>.<lambda> at 0x11056c040>, {((0,), (4, 3, 2)): array([0.42857143, 1.        ]), ((1,), (2, 4, 3)): array([0., 0.]), ((1,), (3, 2, 0)): array([0.        , 0.83333333]), ((3,), (0, 1, 4)): array([0.25, 0.8 ]), ((0,), (1, 4, 2)): array([0.5, 0.8]), ((2,), (3, 4, 1)): array([0.33333333, 1.        ]), ((3,), (2, 0, 1)): array([0.11111111, 1.        ]), ((2,), (1, 4, 0)): array([0., 1.]), ((2,), (0, 4, 3)): array([0.5       , 0.85714286]), ((4,), (2, 1, 3)): array([0., 1.]), ((3,), (2, 4, 0)): array([0.33333333, 0.85714286]), ((4,), (0, 2, 1)): array([0.25, 1.  ]), ((3,), (4, 2, 0)): array([0.22222222, 0.71428571]), ((2,), (1, 0, 3)): array([0.25, 0.5 ]), ((0,), (2, 1, 4)): array([0., 1.]), ((1,), (2, 0, 4)): array([0.28571429, 1.        ]), ((3,), (1, 2, 4)): array([1.   , 0.875]), ((4,), (2, 0, 1)): array([0., 1.]), ((2,), (0, 3, 4)): array([0.  , 0.75]), ((0,), (1, 2, 3)): array([0.        , 0.83333333]), ((0,), (3, 4, 2)): array([0.25, 0.5 ]), ((2,), (0, 1, 3)): array([0.28571429, 0.5       ]), ((2,), (0, 1, 4)): array([0. , 0.5]), ((3,), (2, 0, 4)): array([0., 1.]), ((2,), (3, 4, 0)): array([0.        , 0.66666667]), ((4,), (1, 3, 0)): array([0.25      , 0.83333333]), ((2,), (1, 4, 3)): array([1. , 0.8]), ((2,), (3, 1, 4)): array([0. , 0.8]), ((0,), (4, 2, 3)): array([0.5, 1. ]), ((0,), (2, 1, 3)): array([0., 1.]), ((1,), (3, 0, 2)): array([0.  , 0.75]), ((2,), (3, 1, 0)): array([0.33333333, 0.85714286]), ((0,), (1, 3, 4)): array([0., 1.]), ((4,), (1, 3, 2)): array([0.        , 0.71428571]), ((2,), (0, 3, 1)): array([0.33333333, 0.8       ]), ((4,), (2, 3, 0)): array([0.5 , 0.25]), ((2,), (4, 3, 1)): array([0.25, 1.  ]), ((1,), (2, 3, 0)): array([0.   , 0.875]), ((0,), (4, 1, 3)): array([0.4  , 0.625]), ((0,), (4, 3, 1)): array([0., 1.]), ((1,), (0, 2, 3)): array([0.25, 0.75]), ((4,), (3, 2, 0)): array([0.57142857, 0.66666667]), ((2,), (3, 0, 1)): array([0., 1.]), ((0,), (2, 4, 1)): array([0.14285714, 1.        ]), ((0,), (3, 2, 4)): array([0.33333333, 0.66666667]), ((3,), (2, 1, 0)): array([0. , 0.8]), ((4,), (2, 1, 0)): array([0.14285714, 1.        ]), ((3,), (0, 4, 2)): array([0.4, 1. ]), ((4,), (0, 3, 2)): array([0., 1.]), ((3,), (1, 0, 4)): array([0.5  , 0.875]), ((3,), (2, 4, 1)): array([0.        , 0.83333333]), ((3,), (0, 2, 1)): array([0.33333333, 1.        ]), ((3,), (1, 4, 2)): array([0., 1.]), ((0,), (2, 3, 4)): array([0.33333333, 0.75      ]), ((2,), (4, 3, 0)): array([0.        , 0.71428571]), ((1,), (0, 2, 4)): array([0.        , 0.66666667]), ((2,), (4, 1, 3)): array([0.16666667, 1.        ]), ((3,), (0, 4, 1)): array([0.2, 1. ]), ((0,), (2, 3, 1)): array([0.11111111, 1.        ]), ((1,), (4, 2, 0)): array([0.4, 0.5]), ((3,), (4, 2, 1)): array([0., 1.]), ((4,), (2, 3, 1)): array([0., 1.]), ((1,), (2, 0, 3)): array([0.25      , 0.83333333]), ((1,), (2, 4, 0)): array([0.2 , 0.75]), ((1,), (0, 3, 4)): array([0.2, 1. ]), ((0,), (1, 3, 2)): array([0.28571429, 0.66666667]), ((3,), (4, 1, 2)): array([0.25, 1.  ]), ((2,), (4, 0, 1)): array([0. , 0.5]), ((3,), (1, 0, 2)): array([0.25      , 0.83333333]), ((4,), (3, 2, 1)): array([0.        , 0.85714286]), ((1,), (4, 0, 3)): array([0.5       , 0.66666667]), ((3,), (0, 1, 2)): array([0., 1.]), ((4,), (0, 3, 1)): array([0.4, 1. ]), ((0,), (2, 4, 3)): array([0.2       , 0.57142857]), ((1,), (0, 3, 2)): array([0.33333333, 0.77777778]), ((1,), (4, 0, 2)): array([0.  , 0.75]), ((4,), (0, 1, 2)): array([0.25 , 0.875]), ((0,), (3, 4, 1)): array([0.14285714, 0.75      ]), ((1,), (3, 4, 2)): array([0.5, 1. ]), ((2,), (1, 3, 4)): array([0.5, 1. ]), ((1,), (3, 4, 0)): array([0.2 , 0.75]), ((2,), (1, 3, 0)): array([0., 1.]), ((1,), (4, 2, 3)): array([0.33333333, 1.        ]), ((1,), (3, 2, 4)): array([0.5, 0.8]), ((4,), (1, 2, 0)): array([0.2, 1. ]), ((4,), (0, 1, 3)): array([0., 1.]), ((3,), (0, 2, 4)): array([0., 1.]), ((0,), (1, 2, 4)): array([0.16666667, 1.        ]), ((2,), (0, 4, 1)): array([0.33333333, 1.        ]), ((0,), (3, 1, 4)): array([0., 1.]), ((2,), (4, 0, 3)): array([0.2, 1. ]), ((0,), (4, 1, 2)): array([0. , 0.5]), ((1,), (4, 3, 2)): array([0.        , 0.33333333]), ((4,), (1, 0, 3)): array([0.        , 0.85714286]), ((0,), (3, 2, 1)): array([0.28571429, 1.        ]), ((1,), (2, 3, 4)): array([0.        , 0.71428571]), ((2,), (4, 1, 0)): array([0.2 , 0.75]), ((3,), (2, 1, 4)): array([0.2       , 0.66666667]), ((1,), (0, 4, 3)): array([0.        , 0.83333333]), ((2,), (3, 0, 4)): array([0.2, 1. ]), ((4,), (3, 0, 2)): array([0.5       , 0.66666667]), ((0,), (1, 4, 3)): array([0.5, 1. ]), ((4,), (1, 0, 2)): array([0.  , 0.75]), ((4,), (1, 2, 3)): array([0. , 0.8]), ((4,), (3, 1, 0)): array([0.22222222, 1.        ]), ((1,), (3, 0, 4)): array([0., 1.]), ((3,), (4, 1, 0)): array([0., 1.]), ((0,), (3, 1, 2)): array([0., 1.]), ((0,), (4, 2, 1)): array([0. , 0.6]), ((4,), (0, 2, 3)): array([0.25, 0.6 ]), ((2,), (1, 0, 4)): array([0.  , 0.75]), ((3,), (1, 2, 0)): array([0.375     , 0.66666667]), ((4,), (3, 1, 2)): array([0., 1.]), ((3,), (4, 0, 1)): array([0.2       , 0.66666667]), ((1,), (4, 3, 0)): array([0., 1.]), ((1,), (0, 4, 2)): array([0.25, 0.5 ]), ((3,), (4, 0, 2)): array([0.4, 0.8]), ((4,), (2, 0, 3)): array([0., 1.]), ((3,), (1, 4, 0)): array([1., 1.]), ((4,), (3, 0, 1)): array([1., 1.])})\n",
      "Policy: {((0,), (4, 3, 2)): np.int64(1), ((1,), (2, 4, 3)): np.int64(0), ((1,), (3, 2, 0)): np.int64(1), ((3,), (0, 1, 4)): np.int64(1), ((0,), (1, 4, 2)): np.int64(1), ((2,), (3, 4, 1)): np.int64(1), ((3,), (2, 0, 1)): np.int64(1), ((2,), (1, 4, 0)): np.int64(1), ((2,), (0, 4, 3)): np.int64(1), ((4,), (2, 1, 3)): np.int64(1), ((3,), (2, 4, 0)): np.int64(1), ((4,), (0, 2, 1)): np.int64(1), ((3,), (4, 2, 0)): np.int64(1), ((2,), (1, 0, 3)): np.int64(1), ((0,), (2, 1, 4)): np.int64(1), ((1,), (2, 0, 4)): np.int64(1), ((3,), (1, 2, 4)): np.int64(0), ((4,), (2, 0, 1)): np.int64(1), ((2,), (0, 3, 4)): np.int64(1), ((0,), (1, 2, 3)): np.int64(1), ((0,), (3, 4, 2)): np.int64(1), ((2,), (0, 1, 3)): np.int64(1), ((2,), (0, 1, 4)): np.int64(1), ((3,), (2, 0, 4)): np.int64(1), ((2,), (3, 4, 0)): np.int64(1), ((4,), (1, 3, 0)): np.int64(1), ((2,), (1, 4, 3)): np.int64(0), ((2,), (3, 1, 4)): np.int64(1), ((0,), (4, 2, 3)): np.int64(1), ((0,), (2, 1, 3)): np.int64(1), ((1,), (3, 0, 2)): np.int64(1), ((2,), (3, 1, 0)): np.int64(1), ((0,), (1, 3, 4)): np.int64(1), ((4,), (1, 3, 2)): np.int64(1), ((2,), (0, 3, 1)): np.int64(1), ((4,), (2, 3, 0)): np.int64(0), ((2,), (4, 3, 1)): np.int64(1), ((1,), (2, 3, 0)): np.int64(1), ((0,), (4, 1, 3)): np.int64(1), ((0,), (4, 3, 1)): np.int64(1), ((1,), (0, 2, 3)): np.int64(1), ((4,), (3, 2, 0)): np.int64(1), ((2,), (3, 0, 1)): np.int64(1), ((0,), (2, 4, 1)): np.int64(1), ((0,), (3, 2, 4)): np.int64(1), ((3,), (2, 1, 0)): np.int64(1), ((4,), (2, 1, 0)): np.int64(1), ((3,), (0, 4, 2)): np.int64(1), ((4,), (0, 3, 2)): np.int64(1), ((3,), (1, 0, 4)): np.int64(1), ((3,), (2, 4, 1)): np.int64(1), ((3,), (0, 2, 1)): np.int64(1), ((3,), (1, 4, 2)): np.int64(1), ((0,), (2, 3, 4)): np.int64(1), ((2,), (4, 3, 0)): np.int64(1), ((1,), (0, 2, 4)): np.int64(1), ((2,), (4, 1, 3)): np.int64(1), ((3,), (0, 4, 1)): np.int64(1), ((0,), (2, 3, 1)): np.int64(1), ((1,), (4, 2, 0)): np.int64(1), ((3,), (4, 2, 1)): np.int64(1), ((4,), (2, 3, 1)): np.int64(1), ((1,), (2, 0, 3)): np.int64(1), ((1,), (2, 4, 0)): np.int64(1), ((1,), (0, 3, 4)): np.int64(1), ((0,), (1, 3, 2)): np.int64(1), ((3,), (4, 1, 2)): np.int64(1), ((2,), (4, 0, 1)): np.int64(1), ((3,), (1, 0, 2)): np.int64(1), ((4,), (3, 2, 1)): np.int64(1), ((1,), (4, 0, 3)): np.int64(1), ((3,), (0, 1, 2)): np.int64(1), ((4,), (0, 3, 1)): np.int64(1), ((0,), (2, 4, 3)): np.int64(1), ((1,), (0, 3, 2)): np.int64(1), ((1,), (4, 0, 2)): np.int64(1), ((4,), (0, 1, 2)): np.int64(1), ((0,), (3, 4, 1)): np.int64(1), ((1,), (3, 4, 2)): np.int64(1), ((2,), (1, 3, 4)): np.int64(1), ((1,), (3, 4, 0)): np.int64(1), ((2,), (1, 3, 0)): np.int64(1), ((1,), (4, 2, 3)): np.int64(1), ((1,), (3, 2, 4)): np.int64(1), ((4,), (1, 2, 0)): np.int64(1), ((4,), (0, 1, 3)): np.int64(1), ((3,), (0, 2, 4)): np.int64(1), ((0,), (1, 2, 4)): np.int64(1), ((2,), (0, 4, 1)): np.int64(1), ((0,), (3, 1, 4)): np.int64(1), ((2,), (4, 0, 3)): np.int64(1), ((0,), (4, 1, 2)): np.int64(1), ((1,), (4, 3, 2)): np.int64(1), ((4,), (1, 0, 3)): np.int64(1), ((0,), (3, 2, 1)): np.int64(1), ((1,), (2, 3, 4)): np.int64(1), ((2,), (4, 1, 0)): np.int64(1), ((3,), (2, 1, 4)): np.int64(1), ((1,), (0, 4, 3)): np.int64(1), ((2,), (3, 0, 4)): np.int64(1), ((4,), (3, 0, 2)): np.int64(1), ((0,), (1, 4, 3)): np.int64(1), ((4,), (1, 0, 2)): np.int64(1), ((4,), (1, 2, 3)): np.int64(1), ((4,), (3, 1, 0)): np.int64(1), ((1,), (3, 0, 4)): np.int64(1), ((3,), (4, 1, 0)): np.int64(1), ((0,), (3, 1, 2)): np.int64(1), ((0,), (4, 2, 1)): np.int64(1), ((4,), (0, 2, 3)): np.int64(1), ((2,), (1, 0, 4)): np.int64(1), ((3,), (1, 2, 0)): np.int64(1), ((4,), (3, 1, 2)): np.int64(1), ((3,), (4, 0, 1)): np.int64(1), ((1,), (4, 3, 0)): np.int64(1), ((1,), (0, 4, 2)): np.int64(1), ((3,), (4, 0, 2)): np.int64(1), ((4,), (2, 0, 3)): np.int64(1), ((3,), (1, 4, 0)): np.int64(0), ((4,), (3, 0, 1)): np.int64(0)}\n",
      "Doors: ['goat', 'goat', 'goat', 'goat', 'car'], Selected Doors: [2, 4], Revealed Doors: [1, 0, 3]\n",
      "State: ((2,), (1, 0, 3)), Action: 1, Reward: 1, Next State: ((2, 4), (1, 0, 3))\n",
      "Episode finished in 1 steps.\n"
     ]
    }
   ],
   "source": [
    "# monty_hall_2.py\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "\n",
    "class MontyHall2:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.action_space = namedtuple('ActionSpace', ['n'])\n",
    "        self.action_space.n = 2  # Two actions: 0 (stay), 1 (switch)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.doors = ['goat'] * 4 + ['car']\n",
    "        random.shuffle(self.doors)\n",
    "        self.selected_doors = [random.randint(0, 4)]\n",
    "        self.revealed_doors = self.reveal_doors()\n",
    "        self.state = (tuple(self.selected_doors), tuple(self.revealed_doors))\n",
    "        return self.state\n",
    "    \n",
    "    def reveal_doors(self):\n",
    "        available_doors = [i for i in range(5) if i not in self.selected_doors and self.doors[i] == 'goat']\n",
    "        revealed_doors = random.sample(available_doors, 3)\n",
    "        return revealed_doors\n",
    "    \n",
    "    def step(self, action):\n",
    "        if action == 1:  # Switch\n",
    "            remaining_doors = [i for i in range(5) if i not in self.selected_doors and i not in self.revealed_doors]\n",
    "            self.selected_doors.append(remaining_doors[0])\n",
    "        reward = 1 if self.doors[self.selected_doors[-1]] == 'car' else 0\n",
    "        done = True\n",
    "        self.state = (tuple(self.selected_doors), tuple(self.revealed_doors))\n",
    "        return self.state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Doors: {self.doors}, Selected Doors: {self.selected_doors}, Revealed Doors: {self.revealed_doors}\")\n",
    "\n",
    "def mc_es(env, num_episodes, gamma=1.0):\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    returns_sum = defaultdict(float)\n",
    "    returns_count = defaultdict(float)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        while True:\n",
    "            next_state, reward, done = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done or len(episode) > 100:\n",
    "                break\n",
    "            state = next_state\n",
    "            action = random.choice(range(env.action_space.n))\n",
    "        \n",
    "        G = 0\n",
    "        for state, action, reward in reversed(episode):\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:episode.index((state, action, reward))]]:\n",
    "                returns_sum[(state, action)] += G\n",
    "                returns_count[(state, action)] += 1.0\n",
    "                Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]\n",
    "    \n",
    "    policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
    "    return Q, policy\n",
    "\n",
    "def visualize_policy(env, policy):\n",
    "    state = env.reset()\n",
    "    steps = 0\n",
    "    while True:\n",
    "        action = policy[state]\n",
    "        next_state, reward, done = env.step(action)\n",
    "        env.render()\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}, Next State: {next_state}\")\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if done or steps > 100:\n",
    "            print(f\"Episode finished in {steps} steps.\")\n",
    "            break\n",
    "\n",
    "# Test and visualize Monty Hall Level 2\n",
    "if __name__ == \"__main__\":\n",
    "    env = MontyHall2()\n",
    "    Q, policy = mc_es(env, num_episodes=1000)\n",
    "    print(\"Q-values:\", Q)\n",
    "    print(\"Policy:\", policy)\n",
    "    visualize_policy(env, policy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les Q-values montrent les récompenses attendues pour chaque combinaison de portes sélectionnées et révélées et donc l'agent suit la politique optimale puis gagne la voiture en changeant de porte, ce qui fait qu'il atteint l'état final en une étape aussi."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

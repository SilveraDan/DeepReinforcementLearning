{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import algorithmes.policy_iteration as pi\n",
    "import algorithmes.sarsa as sa\n",
    "import environnements.lineworld as lw\n",
    "import environnements.gridworld as gw\n",
    "from utils import load_config\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4724dc99beb91cbe",
   "metadata": {},
   "source": [
    "congig_file = \"config.yaml\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d0134907eae0680",
   "metadata": {},
   "source": [
    "# LineWorld Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3c9db55068aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create LineWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "de8834985fbb2b56",
   "metadata": {},
   "source": [
    "config_lineworld = load_config(congig_file, \"LineWorld\")\n",
    "game = \"lineworld\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e872c3d22c44f60",
   "metadata": {},
   "source": [
    "S = config_lineworld[\"states\"]\n",
    "A = config_lineworld[\"actions\"]\n",
    "R = config_lineworld[\"rewards\"]\n",
    "T = config_lineworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81ba099a53929f91",
   "metadata": {},
   "source": [
    "lineworld_mdp = lw.create_lineworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7aa5d908c0b6de3",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dd802abcfecc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "c82703c8a3d27776",
   "metadata": {},
   "source": [
    "policy, V = pi.policy_iteration(game, lineworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64259603-9d91-40ce-aa48-ef0471add9b1",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "e702fd38fcda9cb8"
  },
  {
   "cell_type": "markdown",
   "id": "8c74a0d42d095d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sarsa\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fabf293983d54458",
   "metadata": {},
   "source": [
    "print(S)\n",
    "print(A)\n",
    "print(R)\n",
    "print(T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a861efbabc86931",
   "metadata": {},
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, lineworld_mdp, T, num_episodes=10, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=1)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c01bc258f9f8ff7",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a28e28fb92173b5e",
   "metadata": {},
   "source": [
    "# GridWorld Environnement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d02f4c33a2f48",
   "metadata": {},
   "source": [
    "## Create GridWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ff0e7d510d85cb7",
   "metadata": {},
   "source": [
    "config_gridworld = load_config(congig_file, \"GridWorld\")\n",
    "game = \"gridworld\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dc998b856b23be7",
   "metadata": {},
   "source": [
    "S = config_gridworld[\"states\"]\n",
    "A = config_gridworld[\"actions\"]\n",
    "R = config_gridworld[\"rewards\"]\n",
    "T = config_gridworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c670fb717812da8a",
   "metadata": {},
   "source": [
    "gridworld_mdp = gw.create_gridworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dff399b071cb583",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeab246465ec52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "6829f3af29338fde",
   "metadata": {},
   "source": [
    "policy, V = pi.policy_iteration(game, gridworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed60895c5a51e044",
   "metadata": {},
   "source": [
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "ddb85cf00a8b6880"
  },
  {
   "cell_type": "markdown",
   "id": "30832f76f8b35ef8",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b0c572be25f3ffd",
   "metadata": {},
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, gridworld_mdp, T, num_episodes=1000, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=6)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61b485fdf0045241",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28dabed0bbf0154",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

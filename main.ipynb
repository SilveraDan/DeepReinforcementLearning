{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:34:15.327427Z",
     "start_time": "2024-07-24T09:34:15.048241Z"
    }
   },
   "source": [
    "import algorithmes.sarsa as sa\n",
    "import algorithmes.dyna_q as dq\n",
    "import algorithmes.q_learning as ql"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c7aa5d908c0b6de3",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dd802abcfecc6",
   "metadata": {},
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c82703c8a3d27776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T22:28:03.167373Z",
     "start_time": "2024-07-18T22:28:03.151792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "__X__\n",
      "_X___\n",
      "X____\n",
      "Steps: [2, np.int64(1), np.int64(0)]\n",
      "Total Reward: -1\n",
      "Iteration: 2\n",
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Steps: [2, np.int64(3), np.int64(4)]\n",
      "Total Reward: 1\n",
      "Optimal Policy:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Value Function:\n",
      "[0.       0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "source": [
    "policy, V = pi.policy_iteration(game, lineworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "64259603-9d91-40ce-aa48-ef0471add9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T22:28:03.183041Z",
     "start_time": "2024-07-18T22:28:03.167447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Final Steps: [2, np.int64(3), np.int64(4)]\n",
      "Final Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702fd38fcda9cb8",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080afc7f-b89d-4161-aef8-f10de574d3af",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3f6af16-42f1-4cd7-9bd1-72ea7227bdb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:35:29.001516Z",
     "start_time": "2024-07-24T09:35:28.993885Z"
    }
   },
   "source": [
    "param_combinations = [\n",
    "    {\"alpha\": alpha, \"epsilon\": epsilon, \"gamma\": 0.999, \"nb_iter\": nb_iter}\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.2]\n",
    "    for epsilon in [0.01, 0.1, 0.2, 0.5]\n",
    "    for nb_iter in [100, 1000, 10000]\n",
    "]\n",
    "\n",
    "games = [\"LineWorld\"]"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "4ae3dca4-eeb3-4726-965c-9814f8287417",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-24T09:36:10.636785Z",
     "start_time": "2024-07-24T09:35:29.587363Z"
    }
   },
   "source": [
    "for game in games:\n",
    "    for params in param_combinations:\n",
    "        results_path = f\"./result/{game}_q_learning.pkl\"\n",
    "        ql.play_game(game, params, results_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 121.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy calculated\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m params \u001B[38;5;129;01min\u001B[39;00m param_combinations:\n\u001B[1;32m      3\u001B[0m     results_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./result/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgame\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_q_learning.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m     \u001B[43mql\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplay_game\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/DeepReinforcementLearning/algorithmes/q_learning.py:71\u001B[0m, in \u001B[0;36mplay_game\u001B[0;34m(game, parameters, results_path)\u001B[0m\n\u001B[1;32m     69\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[1;32m     70\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mPolicy calculated\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m---> 71\u001B[0m \u001B[43mplay_a_game_by_Pi\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mPi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     72\u001B[0m scored \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mscore()\n\u001B[1;32m     73\u001B[0m save_results_to_pickle(Q_optimal, Pi, scored, results_path)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/DeepReinforcementLearning/utils.py:57\u001B[0m, in \u001B[0;36mplay_a_game_by_Pi\u001B[0;34m(env, Pi)\u001B[0m\n\u001B[1;32m     55\u001B[0m         random_move\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 57\u001B[0m         \u001B[43menv\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     59\u001B[0m     a \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39mchoice(env\u001B[38;5;241m.\u001B[39mavailable_actions())\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/DeepReinforcementLearning/environnements/lineworld.py:63\u001B[0m, in \u001B[0;36mLineWorld.step\u001B[0;34m(self, action)\u001B[0m\n\u001B[1;32m     60\u001B[0m     is_end \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mterminals\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m is_end\n\u001B[0;32m---> 63\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mstep\u001B[39m(\u001B[38;5;28mself\u001B[39m, action: \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m     64\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m action \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     65\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "8c74a0d42d095d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sarsa\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fabf293983d54458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:34:28.031759Z",
     "start_time": "2024-07-23T22:34:28.016133Z"
    }
   },
   "source": [
    "param_combinations = [\n",
    "    {\"alpha\": alpha, \"epsilon\": epsilon, \"gamma\": 0.999, \"nb_iter\": nb_iter}\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.2]\n",
    "    for epsilon in [0.01, 0.1, 0.2, 0.5]\n",
    "    for nb_iter in [100, 1000, 10000, 100000]\n",
    "]\n",
    "\n",
    "games = [\"LineWorld\", \"GridWorld\", \"SecretEnv0\", \"SecretEnv1\"]"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2a861efbabc86931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T22:34:31.888692Z",
     "start_time": "2024-07-23T22:34:28.662599Z"
    }
   },
   "source": [
    "for game in games:\n",
    "    for params in param_combinations:\n",
    "        results_path = f\"./result/{game}_q_learning.pkl\"\n",
    "        sa.play_game(game, params, results_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 9517.80it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 63970.72it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 53061.76it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 115265.37it/s]\n",
      "100%|██████████| 100/100 [00:00<?, ?it/s]\n",
      "100%|██████████| 1000/1000 [00:00<?, ?it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 106223.90it/s]\n",
      "100%|██████████| 100000/100000 [00:00<00:00, 112993.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 6400.59it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 42320.11it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 106228.21it/s]\n",
      " 76%|███████▋  | 76357/100000 [00:00<00:00, 110834.19it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m params \u001B[38;5;129;01min\u001B[39;00m param_combinations:\n\u001B[0;32m      3\u001B[0m     results_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./result/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgame\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_q_learning.pkl\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m----> 4\u001B[0m     \u001B[43msa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplay_game\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresults_path\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\algorithmes\\sarsa.py:62\u001B[0m, in \u001B[0;36mplay_game\u001B[1;34m(game, parameters, results_path)\u001B[0m\n\u001B[0;32m     60\u001B[0m         \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGame not found\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     61\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m---> 62\u001B[0m Q_optimal \u001B[38;5;241m=\u001B[39m \u001B[43msarsa\u001B[49m\u001B[43m(\u001B[49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnb_iter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     63\u001B[0m Pi \u001B[38;5;241m=\u001B[39m calcul_policy(Q_optimal)\n\u001B[0;32m     64\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\algorithmes\\sarsa.py:27\u001B[0m, in \u001B[0;36msarsa\u001B[1;34m(env, alpha, epsilon, gamma, nb_iter)\u001B[0m\n\u001B[0;32m     25\u001B[0m Q \u001B[38;5;241m=\u001B[39m update_Q(Q, s_prime, available_actions_prime, env)\n\u001B[0;32m     26\u001B[0m \u001B[38;5;66;03m# Choose A' from S' using policy derived from Q\u001B[39;00m\n\u001B[1;32m---> 27\u001B[0m a_prime \u001B[38;5;241m=\u001B[39m \u001B[43mchoose_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mQ\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms_prime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mavailable_actions_prime\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Q(s,a) <- Q(s,a) + alpha * [R + gamma * Q(s',a') - Q(s,a)]\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m env\u001B[38;5;241m.\u001B[39mis_game_over():\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\utils.py:74\u001B[0m, in \u001B[0;36mchoose_action\u001B[1;34m(Q, s, available_actions, epsilon)\u001B[0m\n\u001B[0;32m     72\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     73\u001B[0m     q_s \u001B[38;5;241m=\u001B[39m [Q[s][a] \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m available_actions]\n\u001B[1;32m---> 74\u001B[0m     best_a_index \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mq_s\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m     a \u001B[38;5;241m=\u001B[39m available_actions[best_a_index]\n\u001B[0;32m     76\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m a\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:1298\u001B[0m, in \u001B[0;36margmax\u001B[1;34m(a, axis, out, keepdims)\u001B[0m\n\u001B[0;32m   1210\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1211\u001B[0m \u001B[38;5;124;03mReturns the indices of the maximum values along an axis.\u001B[39;00m\n\u001B[0;32m   1212\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1295\u001B[0m \u001B[38;5;124;03m(2, 1, 4)\u001B[39;00m\n\u001B[0;32m   1296\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1297\u001B[0m kwds \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkeepdims\u001B[39m\u001B[38;5;124m'\u001B[39m: keepdims} \u001B[38;5;28;01mif\u001B[39;00m keepdims \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39m_NoValue \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m-> 1298\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _wrapfunc(a, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124margmax\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39maxis, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:54\u001B[0m, in \u001B[0;36m_wrapfunc\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     52\u001B[0m bound \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(obj, method, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m bound \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _wrapit(obj, method, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     56\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     57\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m bound(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\.venv\\lib\\site-packages\\numpy\\_core\\fromnumeric.py:46\u001B[0m, in \u001B[0;36m_wrapit\u001B[1;34m(obj, method, *args, **kwds)\u001B[0m\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# As this already tried the method, subok is maybe quite reasonable here\u001B[39;00m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m# but this follows what was done before. TODO: revisit this.\u001B[39;00m\n\u001B[0;32m     45\u001B[0m arr, \u001B[38;5;241m=\u001B[39m conv\u001B[38;5;241m.\u001B[39mas_arrays(subok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m---> 46\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(arr, method)(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwds)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m conv\u001B[38;5;241m.\u001B[39mwrap(result, to_scalar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "ce9e994aa5bc924e",
   "metadata": {},
   "source": [
    "## Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b31fc4570cf6e0",
   "metadata": {},
   "source": [
    "### Dyna-Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa2789242c15b996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T22:28:03.638480Z",
     "start_time": "2024-07-18T22:28:03.230170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Value Function:\n",
      "[[ 0.        0.      ]\n",
      " [-1.        0.998001]\n",
      " [ 0.997003  0.999   ]\n",
      " [ 0.998001  1.      ]\n",
      " [ 0.        0.      ]]\n"
     ]
    }
   ],
   "source": [
    "param_combinations = [\n",
    "    {\"alpha\": alpha, \"epsilon\": epsilon, \"gamma\": 0.999, \"nb_iter\": nb_iter, \"n_planning\": n_planning}\n",
    "    for alpha in [0.01, 0.05, 0.1, 0.2]\n",
    "    for epsilon in [0.01, 0.1, 0.2, 0.5]\n",
    "    for nb_iter in [100, 1000, 10000, 100000]\n",
    "    for n_planning in [1, 10, 100, 500]\n",
    "    \n",
    "]\n",
    "\n",
    "games = [\"LineWorld\", \"GridWorld\", \"SecretEnv0\", \"SecretEnv1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4268bd962adec2c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T22:28:03.653304Z",
     "start_time": "2024-07-18T22:28:03.638480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Final Steps: [2, np.int64(3), np.int64(4)]\n",
      "Final Total Reward: 1\n"
     ]
    }
   ],
   "source": [
    "for game in games:\n",
    "    for params in param_combinations:\n",
    "        results_path = f\"./result/{game}_dyna_q.pkl\"\n",
    "        dq.play_game(game, params, results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28dabed0bbf0154",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4b4fbfb52b6a0d3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

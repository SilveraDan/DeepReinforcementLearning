{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:24.284629Z",
     "start_time": "2024-07-18T21:13:24.219149Z"
    }
   },
   "source": [
    "import algorithmes.policy_iteration as pi\n",
    "import algorithmes.sarsa as sa\n",
    "import environnements.lineworld as lw\n",
    "import environnements.gridworld as gw\n",
    "from utils import load_config"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "4724dc99beb91cbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:24.428671Z",
     "start_time": "2024-07-18T21:13:24.412581Z"
    }
   },
   "source": [
    "congig_file = \"config.yaml\""
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "6d0134907eae0680",
   "metadata": {},
   "source": [
    "# LineWorld Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3c9db55068aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create LineWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "de8834985fbb2b56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:24.965121Z",
     "start_time": "2024-07-18T21:13:24.949460Z"
    }
   },
   "source": [
    "config_lineworld = load_config(congig_file, \"LineWorld\")\n",
    "game = \"lineworld\""
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "7e872c3d22c44f60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:25.157080Z",
     "start_time": "2024-07-18T21:13:25.137087Z"
    }
   },
   "source": [
    "S = config_lineworld[\"states\"]\n",
    "A = config_lineworld[\"actions\"]\n",
    "R = config_lineworld[\"rewards\"]\n",
    "T = config_lineworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "81ba099a53929f91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:25.333127Z",
     "start_time": "2024-07-18T21:13:25.317074Z"
    }
   },
   "source": [
    "lineworld_mdp = lw.create_lineworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c7aa5d908c0b6de3",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dd802abcfecc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "c82703c8a3d27776",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:26.031286Z",
     "start_time": "2024-07-18T21:13:26.006991Z"
    }
   },
   "source": [
    "policy, V = pi.policy_iteration(game, lineworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "__X__\n",
      "_X___\n",
      "X____\n",
      "Steps: [2, np.int64(1), np.int64(0)]\n",
      "Total Reward: -1\n",
      "Iteration: 2\n",
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Steps: [2, np.int64(3), np.int64(4)]\n",
      "Total Reward: 1\n",
      "Optimal Policy:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Value Function:\n",
      "[0.       0.998001 0.999    1.       0.      ]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "64259603-9d91-40ce-aa48-ef0471add9b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:26.722130Z",
     "start_time": "2024-07-18T21:13:26.707100Z"
    }
   },
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Final Steps: [2, np.int64(3), np.int64(4)]\n",
      "Final Total Reward: 1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "e702fd38fcda9cb8"
  },
  {
   "cell_type": "markdown",
   "id": "8c74a0d42d095d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sarsa\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fabf293983d54458",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:28.541612Z",
     "start_time": "2024-07-18T21:13:28.526906Z"
    }
   },
   "source": [
    "print(S)\n",
    "print(A)\n",
    "print(R)\n",
    "print(T)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, 1]\n",
      "[-1, 0, 1]\n",
      "[0, 4]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "2a861efbabc86931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:28.991115Z",
     "start_time": "2024-07-18T21:13:28.977134Z"
    }
   },
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, lineworld_mdp, T, num_episodes=10, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=1)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "Optimal Policy:\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]]\n",
      "Value Function:\n",
      "[[ 0.          0.        ]\n",
      " [-0.3439      0.01581832]\n",
      " [-0.00999     0.11415074]\n",
      " [ 0.          0.468559  ]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "5c01bc258f9f8ff7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:31.010247Z",
     "start_time": "2024-07-18T21:13:30.987134Z"
    }
   },
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__X__\n",
      "___X_\n",
      "____X\n",
      "Final Steps: [2, np.int64(3), np.int64(4)]\n",
      "Final Total Reward: 1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a28e28fb92173b5e",
   "metadata": {},
   "source": [
    "# GridWorld Environnement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d02f4c33a2f48",
   "metadata": {},
   "source": [
    "## Create GridWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ff0e7d510d85cb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:39.652824Z",
     "start_time": "2024-07-18T21:13:39.637036Z"
    }
   },
   "source": [
    "config_gridworld = load_config(congig_file, \"GridWorld\")\n",
    "game = \"gridworld\""
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "8dc998b856b23be7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:39.984293Z",
     "start_time": "2024-07-18T21:13:39.977068Z"
    }
   },
   "source": [
    "S = config_gridworld[\"states\"]\n",
    "A = config_gridworld[\"actions\"]\n",
    "R = config_gridworld[\"rewards\"]\n",
    "T = config_gridworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "c670fb717812da8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:13:40.333608Z",
     "start_time": "2024-07-18T21:13:40.327252Z"
    }
   },
   "source": [
    "gridworld_mdp = gw.create_gridworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7dff399b071cb583",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeab246465ec52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6829f3af29338fde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:07:35.438441Z",
     "start_time": "2024-07-18T21:07:33.882653Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "_ _ _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "******************************\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "Steps: [12, np.int64(7), np.int64(2)]\n",
      "Total Reward: -1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[15], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m policy, V \u001B[38;5;241m=\u001B[39m \u001B[43mpi\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy_iteration\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgame\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgridworld_mdp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.999\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimal Policy:\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(policy)\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\algorithmes\\policy_iteration.py:41\u001B[0m, in \u001B[0;36mpolicy_iteration\u001B[1;34m(game, P, S, A, R, T, gamma)\u001B[0m\n\u001B[0;32m     39\u001B[0m iteration \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     40\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m---> 41\u001B[0m     V \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_evaluation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mP\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     42\u001B[0m     new_policy, policy_stable \u001B[38;5;241m=\u001B[39m policy_improvement(policy, V, P, S, A, R, gamma)\n\u001B[0;32m     43\u001B[0m     iteration \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\Documents\\PycharmProjects\\DeepReinforcementLearning\\algorithmes\\policy_iteration.py:14\u001B[0m, in \u001B[0;36mpolicy_evaluation\u001B[1;34m(policy, P, S, R, gamma, theta)\u001B[0m\n\u001B[0;32m      9\u001B[0m delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m s \u001B[38;5;129;01min\u001B[39;00m S:\n\u001B[0;32m     11\u001B[0m     v \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(policy[s][a] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28msum\u001B[39m(P[s, a, s_p, r] \u001B[38;5;241m*\u001B[39m (R[r] \u001B[38;5;241m+\u001B[39m gamma \u001B[38;5;241m*\u001B[39m V[s_p])\n\u001B[0;32m     12\u001B[0m                                 \u001B[38;5;28;01mfor\u001B[39;00m s_p \u001B[38;5;129;01min\u001B[39;00m S\n\u001B[0;32m     13\u001B[0m                                 \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(R)))\n\u001B[1;32m---> 14\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpolicy\u001B[49m\u001B[43m[\u001B[49m\u001B[43ms\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m     15\u001B[0m     delta \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmax\u001B[39m(delta, \u001B[38;5;28mabs\u001B[39m(v \u001B[38;5;241m-\u001B[39m V[s]))\n\u001B[0;32m     16\u001B[0m     V[s] \u001B[38;5;241m=\u001B[39m v\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "policy, V = pi.policy_iteration(game, gridworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed60895c5a51e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "ddb85cf00a8b6880"
  },
  {
   "cell_type": "markdown",
   "id": "30832f76f8b35ef8",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b0c572be25f3ffd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:14:29.596708Z",
     "start_time": "2024-07-18T21:14:29.556862Z"
    }
   },
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, gridworld_mdp, T, num_episodes=1000, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=6)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4468\n",
      "Optimal Policy:\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "Value Function:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [-0.9282102   0.49655813  0.70857835 -0.97218716]\n",
      " [-0.92023356  0.49510014  0.7548882   0.63541024]\n",
      " [-0.95760884  0.98801749 -0.9282102   0.72415366]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.68547361  0.          0.         -0.19      ]\n",
      " [ 0.70659938  0.          0.18981     0.05799587]\n",
      " [ 0.8219198   1.         -0.91137062  0.49919292]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.0471033   0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "61b485fdf0045241",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-18T21:14:34.870188Z",
     "start_time": "2024-07-18T21:14:34.856925Z"
    }
   },
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_ _ _ _ _\n",
      "_ X _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "_ _ _ _ _\n",
      "_ _ X _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "******************************\n",
      "_ _ _ _ _\n",
      "_ _ _ X _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "******************************\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ X _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "******************************\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ _ _\n",
      "_ _ _ X _\n",
      "_ _ _ _ _\n",
      "******************************\n",
      "Final Steps: [6, np.int64(7), np.int64(8), np.int64(13), np.int64(18)]\n",
      "Final Total Reward: 1\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "b28dabed0bbf0154",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

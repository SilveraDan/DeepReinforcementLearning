{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {},
   "source": [
    "import algorithmes.policy_iteration as pi\n",
    "import algorithmes.sarsa as sa\n",
    "import algorithmes.dyna_q as dq\n",
    "import algorithmes.dyna_q_plus as dqp\n",
    "import environnements.lineworld as lw\n",
    "import environnements.gridworld as gw\n",
    "from utils import load_config\n",
    "import secret_envs_wrapper as envs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4724dc99beb91cbe",
   "metadata": {},
   "source": [
    "congig_file = \"config.yaml\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6d0134907eae0680",
   "metadata": {},
   "source": [
    "# LineWorld Environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6a3c9db55068aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Create LineWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "de8834985fbb2b56",
   "metadata": {},
   "source": [
    "config_lineworld = load_config(congig_file, \"LineWorld\")\n",
    "game = \"lineworld\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e872c3d22c44f60",
   "metadata": {},
   "source": [
    "S = config_lineworld[\"states\"]\n",
    "A = config_lineworld[\"actions\"]\n",
    "R = config_lineworld[\"rewards\"]\n",
    "T = config_lineworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "81ba099a53929f91",
   "metadata": {},
   "source": [
    "lineworld_mdp = lw.create_lineworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c7aa5d908c0b6de3",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48dd802abcfecc6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "c82703c8a3d27776",
   "metadata": {},
   "source": [
    "policy, V = pi.policy_iteration(game, lineworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "64259603-9d91-40ce-aa48-ef0471add9b1",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "e702fd38fcda9cb8"
  },
  {
   "cell_type": "markdown",
   "id": "8c74a0d42d095d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Sarsa\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fabf293983d54458",
   "metadata": {},
   "source": [
    "print(S)\n",
    "print(A)\n",
    "print(R)\n",
    "print(T)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a861efbabc86931",
   "metadata": {},
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, lineworld_mdp, T, num_episodes=10, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=1)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5c01bc258f9f8ff7",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Planning",
   "id": "ce9e994aa5bc924e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dyna-Q",
   "id": "68b31fc4570cf6e0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy, Q = dq.dyna_q(S, A, R, lineworld_mdp, T, n_episodes = 100, n_planning_steps = 100, alpha = 0.1, gamma = 0.999, epsilon = 0.1, start_state = 1)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "id": "aa2789242c15b996",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "id": "4268bd962adec2c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dyna-Q+",
   "id": "61e46294a532fac"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy, Q = dqp.dyna_q_plus(S, A, R, lineworld_mdp, T, n_episodes = 500, n_planning_steps = 300, alpha = 0.1, gamma = 0.999, k = 0.03, epsilon = 0.1, start_state = 1)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "id": "48104e77a5af3467",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = lw.play_game(policy, lineworld_mdp, R, T)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "id": "cb1f14f890f0bb38",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a28e28fb92173b5e",
   "metadata": {},
   "source": [
    "# GridWorld Environnement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9d02f4c33a2f48",
   "metadata": {},
   "source": [
    "## Create GridWorld environnement"
   ]
  },
  {
   "cell_type": "code",
   "id": "5ff0e7d510d85cb7",
   "metadata": {},
   "source": [
    "config_gridworld = load_config(congig_file, \"GridWorld\")\n",
    "game = \"gridworld\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8dc998b856b23be7",
   "metadata": {},
   "source": [
    "S = config_gridworld[\"states\"]\n",
    "A = config_gridworld[\"actions\"]\n",
    "R = config_gridworld[\"rewards\"]\n",
    "T = config_gridworld[\"terminals\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c670fb717812da8a",
   "metadata": {},
   "source": [
    "gridworld_mdp = gw.create_gridworld(S, A, R)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7dff399b071cb583",
   "metadata": {},
   "source": [
    "## Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eeab246465ec52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "id": "6829f3af29338fde",
   "metadata": {},
   "source": [
    "policy, V = pi.policy_iteration(game, gridworld_mdp, S, A, R, T, gamma=0.999)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(V)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ed60895c5a51e044",
   "metadata": {},
   "source": [
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Temporal Difference Learning",
   "id": "ddb85cf00a8b6880"
  },
  {
   "cell_type": "markdown",
   "id": "30832f76f8b35ef8",
   "metadata": {},
   "source": [
    "### Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "id": "4b0c572be25f3ffd",
   "metadata": {},
   "source": [
    "policy, Q = sa.sarsa(game, S, A, R, gridworld_mdp, T, num_episodes=1000, gamma=0.999, alpha=0.1, epsilon=0.1, start_state=6)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "61b485fdf0045241",
   "metadata": {},
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b28dabed0bbf0154",
   "metadata": {},
   "source": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Planning",
   "id": "9ba42049c0d9524f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dyna-Q",
   "id": "69ce5593b4a0d130"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy, Q = dq.dyna_q(S, A, R, gridworld_mdp, T, n_episodes = 100, n_planning_steps = 100, alpha = 0.1, gamma = 0.999, epsilon = 0.1, start_state = 6)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "id": "cbedf513c45a682d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "id": "618c2beac14c252b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dyna-Q+",
   "id": "e485b5589483d2c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "policy, Q = dqp.dyna_q_plus(S, A, R, gridworld_mdp, T, n_episodes = 100, n_planning_steps = 100, alpha = 0.1, gamma = 0.999, k = 0.5, epsilon = 0.1, start_state = 6)\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy)\n",
    "print(\"Value Function:\")\n",
    "print(Q)"
   ],
   "id": "e449246858213d3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Play the game with the optimal policy\n",
    "steps, total_reward = gw.play_game(policy, gridworld_mdp, R, T, 6)\n",
    "print(f\"Final Steps: {steps}\")\n",
    "print(f\"Final Total Reward: {total_reward}\")"
   ],
   "id": "8a2a17760d33c4b9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Secrets Environnements",
   "id": "d4b4fbfb52b6a0d3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Secret Environnement 1 ",
   "id": "2933d6a00b9f0769"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3dca1bc5bc21416c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f18aa69f86ef0044",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "\n",
   "id": "efce7c5183c63101",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "7a91f9ab6c8896a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e2264813a67c8984",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
